{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25e5fe55",
   "metadata": {},
   "source": [
    "This code was generated with LLM assistance as described in the paper \"Leveraging Large Language Models for Academic Conference Organization\" (see Citation in README). When using the LLM to write initial Python code for paper-reviewer matching, the LLM was only instructed to generate generic code and did not access any specific paper or reviewer data. It neither viewed nor directly matched any paper with a reviewer. After that the conference organizers have revised the code and run the code on AMIA_PC_domains_keywords.csv and AMIA_submission.csv. We were unable to share those files as they contain proprietary information under AMIA terms. However, for use with a future conference, the code should be easily adaptable (e.g., updating the csv header).\n",
    "\n",
    "Note that the process is human-in-the-loop, with close monitoring, interaction, and necessary revisions from the conference chair and vice chairs and AMIA staff on using LLM assisted code for automation tasks such as paper reviewer assignment and duplicate paper detection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5611850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98c5329",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = \"\"\"Advanced data visualization tools and techniques\n",
    "Bioimaging techniques and applications\n",
    "Biomarker discovery and development\n",
    "Biomedical informatics and data science workforce education\n",
    "Citizen Science and democratization of AI and informatics\n",
    "Clinical decision support for translational/data science interventions\n",
    "Clinical genomics/omics and interventions based on omics data\n",
    "Clinical and research data collection, curation, preservation, or sharing\n",
    "Clinical trials innovations\n",
    "Cohort discovery\n",
    "Collaborative workflow systems\n",
    "Data commons\n",
    "Data-driven research and discovery\n",
    "Data Integration\n",
    "Data Literacy and numeracy\n",
    "Data/system integration, standardization and interoperability\n",
    "Data mining and knowledge discovery\n",
    "Data quality\n",
    "Data security and privacy\n",
    "Data sharing / interoperability\n",
    "Data standards\n",
    "Data transformation/ETL\n",
    "Digital research enterprise\n",
    "Drug discovery, repurposing, and side-effect discovery\n",
    "Education and Training\n",
    "EHR-based phenotyping\n",
    "Enterprise data warehouse/Data lake\n",
    "Epigenomics\n",
    "Ethical, legal, and social issues\n",
    "Exposome and data integration\n",
    "Fairness and disparity research in health informatics\n",
    "FHIR\n",
    "Genomics/Omic data interpretation\n",
    "Genotype-phenotype association studies (including GWAS)\n",
    "Geographical information systems (GIS)\n",
    "Health Information and biomedical data dissemination strategies\n",
    "Health literacy issues and solutions\n",
    "Implementation Science\n",
    "Infectious disease modeling\n",
    "Influence of informatics on pharma and insurance industry\n",
    "Informatics for cancer immunotherapy\n",
    "Informatics research/biomedical informatics research methods\n",
    "Integrating omics, clinical, and imaging data\n",
    "Integrative omic analysis\n",
    "Knowledge representation, management, or engineering\n",
    "Learning healthcare system\n",
    "Machine learning and predictive modeling\n",
    "Medical Imaging\n",
    "Measuring outcomes\n",
    "Mobile Health, wearable devices and patient-generated health data\n",
    "Natural Language Processing\n",
    "Ontologies\n",
    "Open Science for biomedical research and translational medicine\n",
    "Outcomes research, clinical epidemiology, population health\n",
    "Patient centered research and care\n",
    "Pharmacogenomics\n",
    "Phenomics and phenome-wide association studies\n",
    "Proactive machine learning and reinforcement learning\n",
    "Public health informatics\n",
    "Real-world evidence and policy making\n",
    "Recruitment technologies\n",
    "Reproducible research methods and tools\n",
    "Single Cell Analysis\n",
    "Secondary use of EHR data\n",
    "Social determinants of health\n",
    "Stakeholder (i.e., patients or community) engagement\n",
    "Sustainable research data infrastructure\n",
    "Systems biology and network analysis\n",
    "Transcriptomics\n",
    "Telehealth and remote care\"\"\"\n",
    "keywords = keywords.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fecb12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "pc = pd.read_csv('AMIA_PC_domains_keywords.csv') # program committee (reviewer) information\n",
    "pc.KEYWORDS_SELECTION = pc.KEYWORDS_SELECTION.apply(literal_eval)\n",
    "pc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9044a329",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission  = pd.read_csv('AMIA_submission.csv') # submission information\n",
    "submission.KEYWORDS_SELECTION = submission.KEYWORDS_SELECTION.apply(literal_eval)\n",
    "submission['diff'] = submission.KEYWORDS_SELECTION.apply(lambda x: set(x).difference(set(keywords)))\n",
    "\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146ccd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "# Function to normalize titles\n",
    "def normalize_title(title):\n",
    "    lower = title.lower()\n",
    "    return lower.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Normalize titles: lowercase and remove punctuation\n",
    "submission['normalized_title'] = submission['submission_TITLE'].apply(normalize_title)\n",
    "\n",
    "# Find exact duplicates\n",
    "title_counts = Counter(submission['normalized_title'])\n",
    "duplicates = {title for title, count in title_counts.items() if count > 1}\n",
    "\n",
    "# Create a column to flag duplicates\n",
    "submission['is_duplicate'] = submission['normalized_title'].apply(lambda x: x in duplicates)\n",
    "\n",
    "# Optionally, display or process the DataFrame with flagged duplicates\n",
    "submission[['submission_TITLE', 'is_duplicate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5797ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Vectorize the titles\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(submission['normalized_title'])\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Flag near-duplicates (adjust threshold as needed)\n",
    "threshold = 0.8  # Example threshold\n",
    "near_duplicates = np.where((cosine_sim > threshold)) #  & (cosine_sim < 1)\n",
    "\n",
    "# Example of processing near-duplicates\n",
    "for i, j in zip(*near_duplicates):\n",
    "    if i < j:\n",
    "        print(f\"Potential near-duplicate pair {submission.iloc[i]['submission_id']}-{submission.iloc[j]['submission_id']}: '{submission.iloc[i]['submission_TITLE']}' and '{submission.iloc[j]['submission_TITLE']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e3afec",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_pc = len(pc)\n",
    "N_submission = len(submission)\n",
    "\n",
    "max_workload = N_submission//N_pc + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802bf2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_pc, N_submission, max_workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f338e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate each PC member max_workload times\n",
    "pc_expanded = pd.DataFrame(\n",
    "    pc.loc[pc.index.repeat(max_workload)].values, \n",
    "    columns=pc.columns\n",
    ")\n",
    "pc_expanded['PC_ID'] = pc_expanded['PC_ID'].astype(str) + '_' + (pc_expanded.groupby('PC_ID').cumcount() + 1).astype(str)\n",
    "pc_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34783aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum Bipartite Matching\n",
    "\n",
    "# Create a bipartite graph\n",
    "B = nx.Graph()\n",
    "\n",
    "# Add nodes with the attribute 'bipartite'\n",
    "B.add_nodes_from(pc_expanded['PC_ID'], bipartite=0)  # PC nodes\n",
    "B.add_nodes_from(submission['submission_id'], bipartite=1)  # Submission nodes\n",
    "\n",
    "# Add edges based on your criteria\n",
    "for _, pc_row in pc_expanded.iterrows():\n",
    "    for _, submission_row in submission.iterrows():\n",
    "        if pc_row['org_domain'] not in submission_row['org_domain']:\n",
    "            # Count overlapping keywords\n",
    "            overlap = len(set(pc_row['KEYWORDS_SELECTION']) & set(submission_row['KEYWORDS_SELECTION']))\n",
    "            if overlap > 0:\n",
    "                # Add an edge with weight\n",
    "                B.add_edge(pc_row['PC_ID'], submission_row['submission_id'], weight=overlap)\n",
    "\n",
    "            \n",
    "# Iterate over the edges in the graph\n",
    "# for (u, v, wt) in B.edges(data='weight'):\n",
    "#     print(f\"Edge from {u} to {v} with weight: {wt}\")\n",
    "\n",
    "# Apply maximum matching\n",
    "max_match = nx.bipartite.maximum_matching(B, top_nodes=set(pc_expanded['PC_ID']))\n",
    "\n",
    "# Filter out only the edges that are from submission to PC\n",
    "filtered_match = {k: v for k, v in max_match.items() if k in set(submission['submission_id'])}\n",
    "print(max_match)\n",
    "\n",
    "print(filtered_match)\n",
    "\n",
    "# Convert the matching result to a DataFrame\n",
    "results_df = pd.DataFrame(list(filtered_match.items()), columns=['submission_id', 'PC_ID'])\n",
    "\n",
    "# Sort the DataFrame based on submission_id\n",
    "results_df.sort_values(by='submission_id', inplace=True)\n",
    "\n",
    "# Reset index of the DataFrame\n",
    "results_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Join results_df with pc_expanded on the PC_ID column\n",
    "# This join will add the FIRST_NAME and LAST_NAME columns to the results_df\n",
    "merged_df = pd.merge(results_df, pc_expanded[['PC_ID', 'FIRST_NAME', 'LAST_NAME']], on='PC_ID', how='left')\n",
    "\n",
    "# Reorder columns if needed and reset index\n",
    "final_df = merged_df[['PC_ID', 'FIRST_NAME', 'LAST_NAME', 'submission_id']]\n",
    "final_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display or use the final DataFrame\n",
    "final_df.to_csv('assignment_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed5354e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Modify the PC_ID column to remove \"_\" and characters after it\n",
    "final_df['PC_ID'] = final_df['PC_ID'].str.split('_').str[0]\n",
    "\n",
    "# Group by PC_ID and collapse submission_id into a list\n",
    "grouped_df = final_df.groupby('PC_ID')['submission_id'].apply(list).reset_index()\n",
    "\n",
    "# Display or use the grouped DataFrame\n",
    "print(grouped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47e25f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by PC_ID and aggregate\n",
    "grouped_df = final_df.groupby('PC_ID').agg({\n",
    "    'submission_id': lambda x: list(x),\n",
    "    'FIRST_NAME': 'first',\n",
    "    'LAST_NAME': 'first'\n",
    "}).reset_index()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
