{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "67ee2e6e-3668-44a0-b5c9-9c36e6129e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "cb96879a-a36e-4902-a769-9b2bae01a88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('accepted_paper_podium_AMIA_IS24.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0e34ef12-1162-4b19-9d00-4e306dc60472",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment for title and abstract\n",
    "texts = ('TITLE: ' + df.submission_TITLE + ' \\n\\nABSTRACT: '+df.submission_ABSTRACT).tolist()\n",
    "#texts = 'TITLE: ' + df.submission_TITLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "a0e1adf0-9ed7-475e-81b5-9142dd082e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwds = df.KEYWORDS_SELECTION.apply(ast.literal_eval).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba593d93-88ba-4707-858a-05d424f75337",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "31debfb3-b6f9-47e8-911b-89f47d755b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api.openai.com/v1/embeddings\"\n",
    "access_token = \"sk-xxx\" # please replace with your openai token\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': f'Bearer {access_token}'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "033b1e1a-72aa-403d-bb86-ca5877c58db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [{\n",
    "    \"input\": t,\n",
    "    \"model\": \"text-embedding-ada-002\"} for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "d7f19254-3e1f-4ae9-8a03-dcd193429efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 144/144 [00:43<00:00,  3.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# embedding for title+abstract\n",
    "gpt_embeds_title_abstract = np.array([requests.post(url, headers=headers, json=inp).json()['data'][0]['embedding'] for inp in tqdm(inputs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "50bb2c46-30b6-4a8f-9c17-60e79147e3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 144/144 [02:22<00:00,  1.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# embedding for keywords\n",
    "gpt_embeds_keywords = []\n",
    "for kl in tqdm(kwds):\n",
    "    embeds = [requests.post(url, headers=headers,\n",
    "                            json={\"input\": k,\"model\": \"text-embedding-ada-002\"}).json()['data'][0]['embedding'] for k in kl]\n",
    "    gpt_embeds_keywords.append(np.array(embeds).mean(axis=0))\n",
    "\n",
    "gpt_embeds_keywords = np.array(gpt_embeds_keywords)      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "53b81285-3b9e-4d40-9c03-8ba1da284d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_embeds = np.concatenate([gpt_embeds_title_abstract, gpt_embeds_keywords],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "86e4d6a2-bc15-4f7f-a339-aa8ad87ae867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install k-means-constrained\n",
    "\n",
    "from k_means_constrained import KMeansConstrained\n",
    "clf = KMeansConstrained(n_clusters=24,size_min=6,size_max=6,random_state=0)\n",
    "df['GPT_cluster'] = clf.fit_predict(np.array(gpt_embeds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35fa2c8-96da-44de-8c17-7b8a2a4fa566",
   "metadata": {},
   "source": [
    "# Llama\n",
    "## https://github.com/SeanLee97/AnglE/tree/cf51cdf011db47c3be6f75b7253c33e11cb3dc81\n",
    "## https://arxiv.org/abs/2309.12871"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "03bd252f-8554-416f-a1ec-63cce1612a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f42c87b23741eca7112cfaf3f262ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ylo7832/.conda/envs/LLAMA/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ylo7832/.conda/envs/LLAMA/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "peft_model_id = 'SeanLee97/angle-llama-7b-nli-v2'\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/data/HF/'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, cache_dir=\"/data/HF/\", do_sample=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, cache_dir=\"/data/HF/\", do_sample=True).bfloat16().cuda()\n",
    "model = PeftModel.from_pretrained(model, peft_model_id, cache_dir=\"/data/HF/\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "dcef1fb8-f4e2-455d-8ade-bbe517809378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLlamaEmbed(txt):\n",
    "    tok = tokenizer([txt], return_tensors='pt')\n",
    "    for k, v in tok.items():\n",
    "        tok[k] = v.cuda()\n",
    "    return model(output_hidden_states=True, **tok).hidden_states[-1][:, -1].float().detach().cpu().numpy()[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "c580d5f6-4b5c-4523-9b9e-a65517ffe535",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 144/144 [00:58<00:00,  2.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# embedding for keywords\n",
    "llama_embeds_title_abstract = [getLlamaEmbed(txt) for txt in tqdm(texts)]\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "95d8ac79-744c-46c7-b020-598b2fece35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 144/144 [01:20<00:00,  1.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# embedding for keywords\n",
    "llama_embeds_keywords = []\n",
    "for kl in tqdm(kwds):\n",
    "    embeds = [getLlamaEmbed(k) for k in kl]\n",
    "    llama_embeds_keywords.append(np.array(embeds).mean(axis=0))\n",
    "\n",
    "llama_embeds_keywords = np.array(llama_embeds_keywords)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "aeca5488-4c8c-4243-9ca4-dbda1aa66117",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_embeds = np.concatenate([llama_embeds_title_abstract, llama_embeds_keywords],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "db67b299-eabe-48ac-81e9-9c1c7c11a922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install k-means-constrained\n",
    "from k_means_constrained import KMeansConstrained\n",
    "clf = KMeansConstrained(n_clusters=24,size_min=6,size_max=6,random_state=0)\n",
    "df['Llama_cluster'] = clf.fit_predict(np.array(llama_embeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "1471fe87-7c77-43e8-a71d-395767e9c494",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('clustered_accepted_paper_podium.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033249a9-4342-458b-b150-a5d9b6202b09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
